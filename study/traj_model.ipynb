{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "os.chdir('../')\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/epinyoan/miniconda3/envs/gmd/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils.model_util import create_model_and_diffusion\n",
    "from eval.eval_humanml import load_dataset\n",
    "import argparse\n",
    "from data_loaders.humanml.scripts.motion_process import recover_from_ric\n",
    "import torch\n",
    "from sample.condition import cond_fn_key_location, get_target_and_inpt_from_kframes_batch\n",
    "import copy\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.parser_util import eval_args\n",
    "# sys.argv = ['/home/epinyoan/git/guided-motion-diffusion/eval/eval_humanml_condition.py', \n",
    "#             '--model_path', \n",
    "#             './save/unet_adazero_xl_x0_abs_proj10_fp16_clipwd_224/model000500000.pt']\n",
    "\n",
    "\n",
    "### 1. args_traj\n",
    "# traj_model_path = \"./save/traj_unet_adazero_swxs_eps_abs_fp16_clipwd_224/model000062500.pt\"\n",
    "# args_traj = eval_args(model_path=traj_model_path)\n",
    "\n",
    "\n",
    "### 2. args\n",
    "# args = eval_args()\n",
    "# args.batch_size = 32 # This must be 32! Don't change it! otherwise it will cause a bug in R precision calc!\n",
    "# args.num_frames = 196 # This must be 196!\n",
    "# args.gen_two_stages = True\n",
    "# skip_first_stage = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# model_path is hard coded: \"./save/traj_unet_adazero_swxs_eps_abs_fp16_clipwd_224/model000062500.pt\"\n",
    "import argparse\n",
    "args = argparse.Namespace(abs_3d=True, action_file='', action_name='', adam_beta2=0.999, apply_zero_mask=False, arch='unet', augment_type='none', avg_model_beta=0, batch_size=32, classifier_scale=100.0, clip_range=6.0, cond_mask_prob=0.1, cuda=True, data_dir='', dataset='humanml', device=0, diffusion_steps=1000, dim_mults=[2, 2, 2, 2], do_inpaint=False, drop_redundant=False, emb_trans_dec=False, eval_batch_size=32, eval_during_training=False, eval_mode='wo_mm', eval_num_samples=1000, eval_rep_times=3, eval_split='test', eval_use_avg=True, ff_size=1024, full_traj_inpaint=False, gen_mse_loss=True, gen_reward_model=False, gen_two_stages=True, grad_clip=0, guidance_mode='no', guidance_param=2.5, impute_until=None, input_text='', interactive=False, lambda_fc=0.0, lambda_rcxyz=0.0, lambda_vel=0.0, latent_dim=512, layers=8, log_interval=1000, lr=0.0001, lr_anneal_steps=0, model_path='./save/unet_adazero_xl_x0_abs_proj10_fp16_clipwd_224/model000500000.pt', motion_length=11.2, motion_length_cut=6.0, noise_schedule='cosine', num_frames=196, num_repetitions=3, num_samples=10, num_steps=600000, out_mult=16, output_dir='', overwrite=False, p2p_impute=True, predict_xstart=True, random_proj_scale=10.0, resume_checkpoint='', save_dir='save/unet_adazero_xl_x0_abs_proj10_fp16_clipwd_224', save_interval=50000, seed=10, sigma_small=True, skip_first=None, std_scale_shift=(1.0, 0.0), text_prompt='', time_weighted_loss=False, train_platform_type='TensorboardPlatform', train_x0_as_eps=False, traj_extra_weight=1.0, traj_only=False, unconstrained=False, unet_adagn=True, unet_zero=True, use_ddim=False, use_fp16=False, use_random_proj=True, weight_decay=0.01, xz_only=False)\n",
    "args_traj = argparse.Namespace(abs_3d=True, action_file='', action_name='', adam_beta2=0.999, apply_zero_mask=False, arch='unet', augment_type='none', avg_model_beta=0, batch_size=512, classifier_scale=100.0, clip_range=6.0, cond_mask_prob=0.1, cuda=True, data_dir='', dataset='humanml', device=0, diffusion_steps=1000, dim_mults=[0.125, 0.25, 0.5], do_inpaint=False, drop_redundant=False, emb_trans_dec=False, eval_batch_size=32, eval_during_training=False, eval_mode='wo_mm', eval_num_samples=1000, eval_rep_times=3, eval_split='test', eval_use_avg=True, ff_size=1024, full_traj_inpaint=False, gen_mse_loss=True, gen_reward_model=False, gen_two_stages=False, grad_clip=0, guidance_mode='no', guidance_param=2.5, impute_until=None, input_text='', interactive=False, lambda_fc=0.0, lambda_rcxyz=0.0, lambda_vel=0.0, latent_dim=512, layers=8, log_interval=125, lr=0.0001, lr_anneal_steps=0, model_path='./save/traj_unet_adazero_swxs_eps_abs_fp16_clipwd_224/model000062500.pt', motion_length=11.2, motion_length_cut=6.0, noise_schedule='cosine', num_frames=60, num_repetitions=3, num_samples=10, num_steps=112500, out_mult=16, output_dir='', overwrite=False, p2p_impute=True, predict_xstart=False, random_proj_scale=10.0, resume_checkpoint='', save_dir='save/traj_unet_adazero_swxs_eps_abs_fp16_clipwd_224', save_interval=6250, seed=10, sigma_small=True, skip_first=None, std_scale_shift=(1.0, 0.0), text_prompt='', time_weighted_loss=False, train_platform_type='TensorboardPlatform', train_x0_as_eps=False, traj_extra_weight=1.0, traj_only=True, unconstrained=False, unet_adagn=True, unet_zero=True, use_ddim=False, use_fp16=False, use_random_proj=False, weight_decay=0.01, xz_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ././dataset/humanml_opt.txt\n",
      "WARNING: max_motion_length is set to 196\n",
      "Loading dataset t2m ...\n",
      "proj_matrix_dir = ./dataset\n",
      "mode = eval\n",
      "t2m dataset aug: none std_scale_shift: (1.0, 0.0)\n",
      "t2m dataset drop redundant information: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4384 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4384/4384 [00:02<00:00, 1815.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer Pointing at 0\n",
      "Loading random projection matrix from ./dataset\n",
      "nfeats: 1\n",
      "Using UNET with lantent dim:  512  and mults:  [0.125, 0.25, 0.5]\n",
      "dims:  [4, 64, 128, 256] mults:  [0.125, 0.25, 0.5]\n",
      "[ models/temporal ] Channel dimensions: [(4, 64), (64, 128), (128, 256)]\n",
      "EMBED TEXT\n",
      "Loading CLIP...\n"
     ]
    }
   ],
   "source": [
    "gen_loader = load_dataset(args, args.num_frames, 'test', hml_mode='eval')\n",
    "traj_model, traj_diffusion = create_model_and_diffusion(args_traj, gen_loader)\n",
    "model_device = 'cuda'\n",
    "traj_model.to(model_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "motion, model_kwargs = next(iter(gen_loader))\n",
    "gt_poses = motion.permute(0, 2, 3, 1)\n",
    "gt_poses = gt_poses * gen_loader.dataset.std + gen_loader.dataset.mean  # [bs, 1, 196, 263]\n",
    "# (x,y,z) [bs, 1, 120, njoints=22, nfeat=3]\n",
    "gt_skel_motions = recover_from_ric(gt_poses.float(), 22, abs_3d=False)\n",
    "gt_skel_motions = gt_skel_motions.view(-1, *gt_skel_motions.shape[2:]).permute(0, 2, 3, 1)\n",
    "gt_skel_motions = traj_model.rot2xyz(x=gt_skel_motions, mask=None, pose_rep='xyz', glob=True, translation=True, \n",
    "                                    jointstype='smpl', vertstrans=True, betas=None, beta=0, glob_rot=None, get_rotations_back=False)\n",
    "\n",
    "n_keyframe = 5\n",
    "sampled_keyframes = torch.rand(motion.shape[0], n_keyframe) * model_kwargs['y']['lengths'].unsqueeze(-1)\n",
    "sampled_keyframes = torch.floor(sampled_keyframes).int().sort()[0]\n",
    "(target, target_mask, \n",
    "inpaint_traj, inpaint_traj_mask,\n",
    "inpaint_traj_points, inpaint_traj_mask_points,\n",
    "inpaint_motion, inpaint_mask, \n",
    "inpaint_motion_points, inpaint_mask_points) = \\\n",
    "    get_target_and_inpt_from_kframes_batch(gt_skel_motions, \n",
    "                                           sampled_keyframes, \n",
    "                                           gen_loader.dataset)\n",
    "target = target.to(model_device)\n",
    "target_mask = target_mask.to(model_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model_kwargs['y']['scale'] = torch.ones(motion.shape[0], device=model_device) * 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "traj_model_kwargs = copy.deepcopy(model_kwargs)\n",
    "traj_model_kwargs['y']['traj_model'] = True\n",
    "traj_model_kwargs['y']['inpainted_motion'] = inpaint_traj.to(model_device) # init_motion.to(model_device)\n",
    "traj_model_kwargs['y']['inpainting_mask'] = inpaint_traj_mask.to(model_device)\n",
    "\n",
    "impute_slack = 20\n",
    "impute_until = 100\n",
    "# Set when to stop imputing\n",
    "traj_model_kwargs['y']['cond_until'] = impute_slack\n",
    "traj_model_kwargs['y']['impute_until'] = impute_until\n",
    "# NOTE: We have the option of switching the target motion from line to just key locations\n",
    "# We call this a 'second stage', which will start after t reach 'impute_until'\n",
    "traj_model_kwargs['y']['impute_until_second_stage'] = impute_slack\n",
    "traj_model_kwargs['y']['inpainted_motion_second_stage'] = inpaint_traj_points.to(model_device)\n",
    "traj_model_kwargs['y']['inpainting_mask_second_stage'] = inpaint_traj_mask_points.to(model_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 102/1000 [00:05<00:49, 18.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900: 3.341138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 202/1000 [00:11<00:45, 17.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800: 3.341138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 302/1000 [00:16<00:38, 17.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700: 3.341138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 402/1000 [00:22<00:33, 17.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600: 3.341138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 502/1000 [00:28<00:27, 18.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500: 3.341138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 602/1000 [00:33<00:21, 18.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400: 3.341138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 701/1000 [00:39<00:14, 20.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300: 3.341138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 802/1000 [00:44<00:11, 17.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200: 3.341138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 902/1000 [00:50<00:05, 17.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100: 3.341138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:55<00:00, 17.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000: 3.341138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sample_fn_traj = traj_diffusion.ddim_sample_loop\n",
    "sample_traj = sample_fn_traj(\n",
    "    traj_model,\n",
    "    inpaint_traj.shape,\n",
    "    clip_denoised=True,\n",
    "    model_kwargs=traj_model_kwargs,  # <-- traj_kwards\n",
    "    skip_timesteps=0,  # NOTE: for debugging, start from 900\n",
    "    init_image=None,\n",
    "    progress=True,\n",
    "    dump_steps=None,\n",
    "    noise=None,\n",
    "    const_noise=False,\n",
    "    cond_fn=partial(\n",
    "        cond_fn_key_location, # cond_fn_sdf, #,\n",
    "        transform=gen_loader.dataset.t2m_dataset.transform_th,\n",
    "        inv_transform=gen_loader.dataset.t2m_dataset.inv_transform_th,\n",
    "        target=target,\n",
    "        target_mask=target_mask,\n",
    "        kframes=[],\n",
    "        abs_3d=True, # <<-- hard code,\n",
    "        classifiler_scale=100.0,\n",
    "        use_mse_loss=False),  # <<-- hard code\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gmd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
